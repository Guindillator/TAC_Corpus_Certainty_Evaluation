{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "from nltk.text import Text\n",
    "from nltk.corpus import state_union\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag.stanford import StanfordNERTagger as st\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.manifold import MDS\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import nltk\n",
    "import re\n",
    "import os  # for os.path.basename\n",
    "#import urllib\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")# est√° entre porter y lancaster stemmer PONER EN LA TESIS EL PORQUE SE HA ELEGIDO ESTE.\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def COSINE(ref):#cit = query sentence ref = reference againts you match // Funcion tokenize_and_stem y tokenize_only va dentro de los parametros de TfidfVectorizer como \"tokenizer = tokenize_and_stem\" o \"tokenizer = tokenize_only\"\n",
    "    pepe = {}\n",
    "    winref = []\n",
    "    documents = ref #before[0:]\n",
    "    tfidf_vectorizer = TfidfVectorizer( stop_words='english',use_idf=True, tokenizer = tokenize_and_stem, min_df = 2)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "    CS = 1 - cosine_similarity(tfidf_matrix)\n",
    "    return CS\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    lemma = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    stems = [stemmer.stem(t) for t in lemma]\n",
    "    return stems\n",
    "\n",
    "QUERY = \"\"\"An alternative format, and one that has been widely used for Cas9-based screens, is pooled screening, in which pooled lentiviral libraries are transduced at a low multiplicity of infection (MOI) to ensure that most cells receive only one stably integrated RNA guide\"\"\"\n",
    "\n",
    "file_txt = open('Statements','r+')\n",
    "x = []\n",
    "for i in file_txt:\n",
    "    if i[0].isdigit() == True:\n",
    "        x.append(i)\n",
    "dist = COSINE(x)\n",
    "\n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "Sentence = \"Sentence - \"\n",
    "Sentence = [Sentence+str(i) for i in range(1,46)]\n",
    "fig, ax = plt.subplots(figsize=(20, 7)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"top\", labels=Sentence);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x')        # changes apply to the x-axis\n",
    "    #which='both',      # both major and minor ticks are affected\n",
    "    #bottom='off',      # ticks along the bottom edge are off\n",
    "    #top='off',         # ticks along the top edge are off\n",
    "    #labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "#uncomment below to save figure\n",
    "plt.savefig('Statements(ward_clusters).png', dpi=300) \n",
    "#---------------------------------------------\n",
    "###  MULTIDIMENSIONAL SCALING MDS\n",
    "#MDS()\n",
    "\n",
    "## convert two components as we're plotting points in a two-dimensional plane\n",
    "## \"precomputed\" because we provide a distance matrix\n",
    "## we will also specify `random_state` o the plot is reproducible.\n",
    "#mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "#pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "#xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
